{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Step 1: Load dataset from Hugging Face\n",
    "dataset = datasets.load_dataset(\"browndw/human-ai-parallel-corpus-biber\", split=\"train\")\n",
    "df = pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check first few rows of `doc_id`\n",
    "print(df[['doc_id']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Preprocess the dataset\n",
    "# Extracting `source` from `doc_id`\n",
    "df['source'] = df['doc_id'].apply(lambda x: x.split(\"@\")[-1])\n",
    "# Extract genre from `doc_id`\n",
    "df['genre'] = df['doc_id'].apply(lambda x: x.split(\"@\")[0].split(\"_\")[0] if isinstance(x, str) else \"unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique sources\n",
    "unique_sources = df['source'].unique()\n",
    "print(unique_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique genres\n",
    "unique_genres = df['genre'].unique()\n",
    "print(unique_genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection\n",
    "feature_cols = [col for col in df.columns if col.startswith('f_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Selected features:\")\n",
    "print(feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing feature values in any row\n",
    "missing_values = df[feature_cols].isnull().any(axis=1)\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(missing_values.sum())  # Number of rows with at least one NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.dropna(subset=feature_cols)  # Drop rows with missing feature values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Remove 'chunk_1' from the dataset\n",
    "df_filtered = df[df['source'] != \"chunk_1\"].copy()\n",
    "\n",
    "# Step 2: Convert source to a categorical data type and encode as numeric AFTER filtering\n",
    "df_filtered['source_encoded'] = df_filtered['source'].astype('category').cat.codes\n",
    "\n",
    "# Step 3: Redefine `source_mapping` AFTER filtering (so it excludes `chunk_1`)\n",
    "source_mapping = dict(enumerate(df_filtered['source'].astype('category').cat.categories))\n",
    "\n",
    "# Step 4: Ensure correct target column\n",
    "X = df_filtered[feature_cols]  # Features\n",
    "y = df_filtered['source_encoded']  # Encoded labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y  # Ensures balanced class distribution\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest Model\n",
    "rf = RandomForestClassifier(n_estimators=500, max_features=8, random_state=42)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training Accuracy: {rf.score(X_train, y_train):.4f}\")\n",
    "print(f\"Test Accuracy: {rf.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Convert numeric predictions back to text labels\n",
    "y_test_labels = y_test.map(source_mapping)  # Actual labels\n",
    "y_pred_labels = pd.Series(y_pred).map(source_mapping)  # Predicted labels\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test_labels, y_pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test_labels, y_pred_labels)\n",
    "labels = sorted(y_test_labels.unique())\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = pd.DataFrame({'Feature': feature_cols, 'Importance': rf.feature_importances_})\n",
    "feature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance[:10], palette=\"Blues_r\")\n",
    "plt.xlabel(\"Feature Importance Score\")\n",
    "plt.ylabel(\"Top Features\")\n",
    "plt.title(\"Top 10 Important Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=1000, max_features=10, max_depth=20, random_state=42)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training Accuracy: {rf.score(X_train, y_train):.4f}\")\n",
    "print(f\"Test Accuracy: {rf.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(\n",
    "    n_estimators=300,  # Reduce trees (was 500+)\n",
    "    max_depth=15,  # Limit tree depth to prevent overfitting\n",
    "    max_features=\"sqrt\",  # Use square root of features for each split\n",
    "    min_samples_split=5,  # Require at least 5 samples to split\n",
    "    min_samples_leaf=3,  # Ensure at least 3 samples in leaf nodes\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training Accuracy: {rf.score(X_train, y_train):.4f}\")\n",
    "print(f\"Test Accuracy: {rf.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=15,  \n",
    "    max_features=\"sqrt\",  \n",
    "    min_samples_split=10,  # More conservative splits\n",
    "    min_samples_leaf=5,  # Ensure at least 5 samples in leaf nodes\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training Accuracy: {rf.score(X_train, y_train):.4f}\")\n",
    "print(f\"Test Accuracy: {rf.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(\n",
    "    n_estimators=200,  \n",
    "    max_depth=15,  \n",
    "    max_features=\"sqrt\",  \n",
    "    min_samples_split=10,  \n",
    "    min_samples_leaf=10,  # Force even smoother leaf nodes  \n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training Accuracy: {rf.score(X_train, y_train):.4f}\")\n",
    "print(f\"Test Accuracy: {rf.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(\n",
    "    n_estimators=200,  \n",
    "    max_depth=12,  # Reduce tree depth for smoother decision boundaries  \n",
    "    max_features=\"sqrt\",  \n",
    "    min_samples_split=10,  \n",
    "    min_samples_leaf=10,  \n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training Accuracy: {rf.score(X_train, y_train):.4f}\")\n",
    "print(f\"Test Accuracy: {rf.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(\n",
    "    n_estimators=200,  \n",
    "    max_depth=12,  \n",
    "    max_features=\"sqrt\",  \n",
    "    min_samples_split=10,  \n",
    "    min_samples_leaf=5,  # Allow finer granularity  \n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training Accuracy: {rf.score(X_train, y_train):.4f}\")\n",
    "print(f\"Test Accuracy: {rf.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(\n",
    "    n_estimators=300,  # More trees for better generalization  \n",
    "    max_depth=12,  \n",
    "    max_features=\"sqrt\",  \n",
    "    min_samples_split=10,  \n",
    "    min_samples_leaf=5,  \n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training Accuracy: {rf.score(X_train, y_train):.4f}\")\n",
    "print(f\"Test Accuracy: {rf.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(\n",
    "    n_estimators=300,  \n",
    "    max_depth=12,  \n",
    "    max_features=\"sqrt\",  \n",
    "    min_samples_split=10,  \n",
    "    min_samples_leaf=5,  \n",
    "    class_weight=\"balanced\",  # Adjusts weights dynamically  \n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training Accuracy: {rf.score(X_train, y_train):.4f}\")\n",
    "print(f\"Test Accuracy: {rf.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': feature_cols, \n",
    "    'Importance': rf.feature_importances_\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "print(feature_importance.head(15))  # Check top 15 important features\n",
    "\n",
    "# Drop low-importance features\n",
    "important_features = feature_importance[feature_importance['Importance'] > 0.005][\"Feature\"]\n",
    "X_train_selected = X_train[important_features]\n",
    "X_test_selected = X_test[important_features]\n",
    "\n",
    "rf.fit(X_train_selected, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select important features for training\n",
    "X_train_selected = X_train[important_features]\n",
    "X_test_selected = X_test[important_features]\n",
    "\n",
    "# Train on selected features\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=200,  \n",
    "    max_depth=12,  \n",
    "    max_features=\"sqrt\",  \n",
    "    min_samples_split=10,  \n",
    "    min_samples_leaf=5,  \n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_selected, y_train)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_accuracy = rf.score(X_test_selected, y_test)\n",
    "print(f\"Test Accuracy (After Feature Selection): {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(\n",
    "    n_estimators=200,  \n",
    "    max_depth=10,  # Reduce tree depth further  \n",
    "    max_features=\"sqrt\",  \n",
    "    min_samples_split=10,  \n",
    "    min_samples_leaf=5,  \n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train_selected, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training Accuracy: {rf.score(X_train_selected, y_train):.4f}\")\n",
    "print(f\"Test Accuracy: {rf.score(X_test_selected, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=200,  \n",
    "    max_depth=10,  \n",
    "    min_samples_split=10,  \n",
    "    min_samples_leaf=5,  \n",
    "    max_features=\"sqrt\",  \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "cv_scores = cross_val_score(rf, X_train_selected, y_train, cv=5)\n",
    "print(f\"Cross-validation Accuracy: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(\n",
    "    n_estimators=200,  \n",
    "    max_depth=15,  \n",
    "    max_features=\"sqrt\",  \n",
    "    min_samples_split=10,  \n",
    "    min_samples_leaf=5,  \n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_accuracy = rf.score(X_test, y_test)\n",
    "print(f\"Test Accuracy (All Features, max_depth=15): {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=200,  \n",
    "    max_depth=10,  \n",
    "    learning_rate=0.05,  # Adjust learning rate  \n",
    "    subsample=0.8,  # Randomly sample 80% of data per tree  \n",
    "    colsample_bytree=0.8,  # Use 80% of features per tree  \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb.fit(X_train, y_train)\n",
    "test_accuracy = xgb.score(X_test, y_test)\n",
    "print(f\"XGBoost Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Define a simple MLP model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(256, activation=\"relu\", input_shape=(X_train.shape[1],)),\n",
    "    keras.layers.Dense(128, activation=\"relu\"),\n",
    "    keras.layers.Dense(len(y_train.unique()), activation=\"softmax\")  # Multi-class classification\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    XGBClassifier(random_state=42),\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=10,  # Randomly test 10 different combinations\n",
    "    cv=3,  # Reduce cross-validation folds\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "print(\"Best Parameters:\", random_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Train XGBoost with best parameters from RandomizedSearchCV\n",
    "xgb_optimized = XGBClassifier(\n",
    "    subsample=0.8,  \n",
    "    n_estimators=300,  \n",
    "    max_depth=8,  \n",
    "    learning_rate=0.1,  \n",
    "    colsample_bytree=0.8,  \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb_optimized.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_accuracy = xgb_optimized.score(X_test, y_test)\n",
    "print(f\"Optimized XGBoost Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Make predictions\n",
    "y_pred = xgb_optimized.predict(X_test)\n",
    "\n",
    "# Convert numeric predictions back to text labels\n",
    "y_test_labels = y_test.map(source_mapping)  \n",
    "y_pred_labels = pd.Series(y_pred).map(source_mapping)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test_labels, y_pred_labels))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test_labels, y_pred_labels)\n",
    "unique_labels = sorted(y_test_labels.unique())\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=unique_labels, yticklabels=unique_labels)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Optimized XGBoost Confusion Matrix\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.yticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Get feature importances\n",
    "feature_importance = pd.DataFrame({\n",
    "    \"Feature\": X_train.columns, \n",
    "    \"Importance\": xgb_optimized.feature_importances_\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# Plot top 10 features\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=\"Importance\", y=\"Feature\", data=feature_importance[:10])\n",
    "plt.xlabel(\"Feature Importance Score\")\n",
    "plt.ylabel(\"Top Features\")\n",
    "plt.title(\"Top 10 Important Features (XGBoost)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create stacking ensemble\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('xgb', XGBClassifier(subsample=0.8, n_estimators=300, max_depth=8, learning_rate=0.1, colsample_bytree=0.8, random_state=42)),\n",
    "        ('rf', RandomForestClassifier(n_estimators=200, max_depth=15, random_state=42))\n",
    "    ],\n",
    "    final_estimator=XGBClassifier(n_estimators=100, random_state=42),  # Meta-model\n",
    "    cv=3\n",
    ")\n",
    "\n",
    "stacking_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "test_accuracy_stacking = stacking_model.score(X_test, y_test)\n",
    "print(f\"Stacking Model Test Accuracy: {test_accuracy_stacking:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_test_labels, y_pred_labels)\n",
    "labels = sorted(y_test_labels.unique())  # Ensure correct label order\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_normalized = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(cm_normalized, annot=True, fmt=\".2f\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Normalized Confusion Matrix\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Get feature importances\n",
    "feature_importance = pd.DataFrame({\n",
    "    \"Feature\": X_train.columns, \n",
    "    \"Importance\": xgb_optimized.feature_importances_\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# Plot top 10 features\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=\"Importance\", y=\"Feature\", data=feature_importance[:10])\n",
    "plt.xlabel(\"Feature Importance Score\")\n",
    "plt.ylabel(\"Top Features\")\n",
    "plt.title(\"Top 10 Important Features (XGBoost)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Create SHAP explainer\n",
    "explainer = shap.Explainer(xgb_optimized)\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "# SHAP Summary Plot\n",
    "shap.summary_plot(shap_values, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.beeswarm(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Aggregate predictions per class\n",
    "predicted_class_counts = pd.Series(y_pred_labels).value_counts(normalize=True)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=predicted_class_counts.index, y=predicted_class_counts.values)\n",
    "plt.xlabel(\"Predicted Class\")\n",
    "plt.ylabel(\"Proportion\")\n",
    "plt.title(\"Prediction Distribution Across LLMs & Humans\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Reduce to 2 components\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_test)\n",
    "\n",
    "# Convert to DataFrame\n",
    "pca_df = pd.DataFrame(X_pca, columns=[\"PC1\", \"PC2\"])\n",
    "pca_df[\"Source\"] = y_test_labels.values\n",
    "\n",
    "# Plot PCA\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=pca_df, x=\"PC1\", y=\"PC2\", hue=\"Source\", alpha=0.7, palette=\"tab10\")\n",
    "plt.title(\"PCA: Classification Separation by Source\")\n",
    "plt.legend(bbox_to_anchor=(1, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Reduce to 5 components\n",
    "pca = PCA(n_components=5)\n",
    "pca.fit(X_test)\n",
    "print(pca.explained_variance_ratio_)  # Check variance captured by each component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(range(1, 6), pca.explained_variance_ratio_, marker='o', linestyle='--')\n",
    "plt.xlabel(\"Principal Component\")\n",
    "plt.ylabel(\"Variance Explained\")\n",
    "plt.title(\"PCA Scree Plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "pca_3d = PCA(n_components=3)\n",
    "X_pca_3d = pca_3d.fit_transform(X_test)\n",
    "pca_df_3d = pd.DataFrame(X_pca_3d, columns=[\"PC1\", \"PC2\", \"PC3\"])\n",
    "pca_df_3d[\"Source\"] = y_test_labels.values\n",
    "\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "scatter = ax.scatter(pca_df_3d[\"PC1\"], pca_df_3d[\"PC2\"], pca_df_3d[\"PC3\"], \n",
    "                     c=y_test.astype(\"category\").cat.codes, cmap=\"tab10\", alpha=0.7)\n",
    "ax.set_xlabel(\"PC1\")\n",
    "ax.set_ylabel(\"PC2\")\n",
    "ax.set_zlabel(\"PC3\")\n",
    "plt.title(\"3D PCA Classification Separation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "X_tsne = TSNE(n_components=2, perplexity=30, random_state=42).fit_transform(X_test)\n",
    "tsne_df = pd.DataFrame(X_tsne, columns=[\"t-SNE1\", \"t-SNE2\"])\n",
    "tsne_df[\"Source\"] = y_test_labels.values\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.scatterplot(data=tsne_df, x=\"t-SNE1\", y=\"t-SNE2\", hue=\"Source\", alpha=0.7, palette=\"tab10\")\n",
    "plt.title(\"t-SNE: Classification Separation by Source\")\n",
    "plt.legend(bbox_to_anchor=(1,1))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
